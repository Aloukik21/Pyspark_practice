{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create a sample DataFrame\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Age': [25, 30, 35],\n",
    "        'City': ['New York', 'Paris', 'London']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# save the DataFrame to a CSV file\n",
    "df.to_csv('data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>35</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name  Age      City\n",
       "0    Alice   25  New York\n",
       "1      Bob   30     Paris\n",
       "2  Charlie   35    London"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d9aeab8790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing header column name\n",
    "df_pyspark = spark.read.option('header',\"true\").csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+\n",
      "|   Name|Age|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|New York|\n",
      "|    Bob| 30|   Paris|\n",
      "|Charlie| 35|  London|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Dataframe\n",
    "## Reading The Dataset\n",
    "## Checking the Datatypes of the Column(Schema)\n",
    "## Selecting Columns And Indexing\n",
    "## Check Describe option similar to Pandas\n",
    "## Adding Columns\n",
    "## Dropping columns\n",
    "## Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|   Name|    City|\n",
      "+-------+--------+\n",
      "|  Alice|New York|\n",
      "|    Bob|   Paris|\n",
      "|Charlie|  London|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','City']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'string'), ('City', 'string')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+------+\n",
      "|summary|   Name| Age|  City|\n",
      "+-------+-------+----+------+\n",
      "|  count|      3|   3|     3|\n",
      "|   mean|   null|30.0|  null|\n",
      "| stddev|   null| 5.0|  null|\n",
      "|    min|  Alice|  25|London|\n",
      "|    max|Charlie|  35| Paris|\n",
      "+-------+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Adding Columns in data frame\n",
    "df_pyspark=df_pyspark.withColumn('Age After 2 year',df_pyspark['Age']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+----------------+\n",
      "|   Name|Age|    City|Age After 2 year|\n",
      "+-------+---+--------+----------------+\n",
      "|  Alice| 25|New York|            27.0|\n",
      "|    Bob| 30|   Paris|            32.0|\n",
      "|Charlie| 35|  London|            37.0|\n",
      "+-------+---+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+--------+----------------+\n",
      "|New Name|Age|    City|Age After 2 year|\n",
      "+--------+---+--------+----------------+\n",
      "|   Alice| 25|New York|            27.0|\n",
      "|     Bob| 30|   Paris|            32.0|\n",
      "| Charlie| 35|  London|            37.0|\n",
      "+--------+---+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Rename the columns\n",
    "df_pyspark.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Handling missing value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id     name   department        title    salary\n",
      "0            1     John           HR      Manager  100000.0\n",
      "1            2    Alice  Engineering     Engineer   90000.0\n",
      "2            3      Bob      Finance          NaN   80000.0\n",
      "3            4  Charlie    Marketing  Coordinator       NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Define the column names for the DataFrame\n",
    "columns = ['employee_id', 'name', 'department', 'title', 'salary']\n",
    "\n",
    "# Define the data for the DataFrame\n",
    "data = [[1, 'John', 'HR', 'Manager', 100000],\n",
    "        [2, 'Alice', 'Engineering', 'Engineer', 90000],\n",
    "        [3, 'Bob', 'Finance', np.nan, 80000],\n",
    "        [4, 'Charlie', 'Marketing', 'Coordinator', np.nan]]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame to a CSV file\n",
    "df.to_csv('employee.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv(\"employee.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+-----------+--------+\n",
      "|employee_id|   name| department|      title|  salary|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "|          1|   John|         HR|    Manager|100000.0|\n",
      "|          2|  Alice|Engineering|   Engineer| 90000.0|\n",
      "|          3|    Bob|    Finance|       null| 80000.0|\n",
      "|          4|Charlie|  Marketing|Coordinator|    null|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+--------+\n",
      "|   name| department|      title|  salary|\n",
      "+-------+-----------+-----------+--------+\n",
      "|   John|         HR|    Manager|100000.0|\n",
      "|  Alice|Engineering|   Engineer| 90000.0|\n",
      "|    Bob|    Finance|       null| 80000.0|\n",
      "|Charlie|  Marketing|Coordinator|    null|\n",
      "+-------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop(\"employee_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------+--------+--------+\n",
      "|employee_id| name| department|   title|  salary|\n",
      "+-----------+-----+-----------+--------+--------+\n",
      "|          1| John|         HR| Manager|100000.0|\n",
      "|          2|Alice|Engineering|Engineer| 90000.0|\n",
      "+-----------+-----+-----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+-----------+--------+\n",
      "|employee_id|   name| department|      title|  salary|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "|          1|   John|         HR|    Manager|100000.0|\n",
      "|          2|  Alice|Engineering|   Engineer| 90000.0|\n",
      "|          3|    Bob|    Finance|       null| 80000.0|\n",
      "|          4|Charlie|  Marketing|Coordinator|    null|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "\n",
      "+-----------+-----+-----------+--------+--------+\n",
      "|employee_id| name| department|   title|  salary|\n",
      "+-----------+-----+-----------+--------+--------+\n",
      "|          1| John|         HR| Manager|100000.0|\n",
      "|          2|Alice|Engineering|Engineer| 90000.0|\n",
      "|          3|  Bob|    Finance|    null| 80000.0|\n",
      "+-----------+-----+-----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## any==how drop if there any null in row\n",
    "## all = drop all rows with all nan in rows\n",
    "## threshold if 2, if atleast 2 null values then it will be deleted\n",
    "## subset = remove from specific col\n",
    "\n",
    "df_pyspark.na.drop(how=\"any\", thresh=2).show()\n",
    "df_pyspark.na.drop(how=\"any\", subset=[\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+-----------+--------+\n",
      "|employee_id|   name| department|      title|  salary|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "|          1|   John|         HR|    Manager|100000.0|\n",
      "|          2|  Alice|Engineering|   Engineer| 90000.0|\n",
      "|          3|    Bob|    Finance|       null| 80000.0|\n",
      "|          4|Charlie|  Marketing|Coordinator|     0.0|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "\n",
      "+-----------+-------+-----------+-------------+--------+\n",
      "|employee_id|   name| department|        title|  salary|\n",
      "+-----------+-------+-----------+-------------+--------+\n",
      "|          1|   John|         HR|      Manager|100000.0|\n",
      "|          2|  Alice|Engineering|     Engineer| 90000.0|\n",
      "|          3|    Bob|    Finance|MIssing_value| 80000.0|\n",
      "|          4|Charlie|  Marketing|  Coordinator|    null|\n",
      "+-----------+-------+-----------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## fill missing value\n",
    "\n",
    "df_pyspark.na.fill(0).show()\n",
    "df_pyspark.na.fill(\"MIssing_value\", subset=[\"title\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+-----------+--------+\n",
      "|employee_id|   name| department|      title|  salary|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "|          1|   John|         HR|    Manager|100000.0|\n",
      "|          2|  Alice|Engineering|   Engineer| 90000.0|\n",
      "|          3|    Bob|    Finance|       null| 80000.0|\n",
      "|          4|Charlie|  Marketing|Coordinator|    null|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing mean\n",
    "# Calculate the mean of each column\n",
    "from pyspark.sql.functions import mean\n",
    "means = df_pyspark.select([mean(c) for c in df_pyspark.columns]).first()\n",
    "# Calculate the mean of the \"salary\" column\n",
    "mean_salary = df_pyspark.select(mean('salary')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(avg(employee_id)=2.5, avg(name)=None, avg(department)=None, avg(title)=None, avg(salary)=90000.0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.na.fill(mean_salary, subset=[\"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+-----------+--------+\n",
      "|employee_id|   name| department|      title|  salary|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "|          1|   John|         HR|    Manager|100000.0|\n",
      "|          2|  Alice|Engineering|   Engineer| 90000.0|\n",
      "|          3|    Bob|    Finance|       null| 80000.0|\n",
      "|          4|Charlie|  Marketing|Coordinator| 90000.0|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter operations\n",
    "df_pyspark.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+-----------+--------+\n",
      "|employee_id|   name| department|      title|  salary|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "|          1|   John|         HR|    Manager|100000.0|\n",
      "|          2|  Alice|Engineering|   Engineer| 90000.0|\n",
      "|          4|Charlie|  Marketing|Coordinator| 90000.0|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#salary of people more than 90k\n",
    "\n",
    "df_pyspark.filter(\"salary>=90000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   Name| department|\n",
      "+-------+-----------+\n",
      "|   John|         HR|\n",
      "|  Alice|Engineering|\n",
      "|Charlie|  Marketing|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(\"salary>=90000\").select([\"Name\",\"department\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----------+-------+--------+\n",
      "|employee_id|name|department|  title|  salary|\n",
      "+-----------+----+----------+-------+--------+\n",
      "|          1|John|        HR|Manager|100000.0|\n",
      "+-----------+----+----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark[\"Salary\"] >= 90000) &\n",
    "                 (df_pyspark[\"Name\"] == \"John\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----------+-----+-------+\n",
      "|employee_id|name|department|title| salary|\n",
      "+-----------+----+----------+-----+-------+\n",
      "|          3| Bob|   Finance| null|80000.0|\n",
      "+-----------+----+----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inverse operation\n",
    "df_pyspark.filter(~(df_pyspark[\"salary\"] >= 90000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by and aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the column names\n",
    "columns = [\"Name\", \"Age\", \"Gender\", \"Department\", \"Experience\", \"Salary\"]\n",
    "\n",
    "# Define the row values\n",
    "rows = [(\"John\", 35, \"Male\", \"Sales\", 13, 90000),\n",
    "        (\"Jane\", 28, \"Female\", \"Marketing\", 6, 75000),\n",
    "        (\"Bob\", 42, \"Male\", \"Engineering\", 20, 120000),\n",
    "        (\"Mary\", 31, \"Female\", \"Sales\", 9, 80000),\n",
    "        (\"Steve\", 45, \"Male\", \"Operations\", 23, 100000),\n",
    "        (\"Emily\", 29, \"Female\", \"Marketing\", 7, 85000),\n",
    "        (\"David\", 39, \"Male\", \"Engineering\", 17, 110000),\n",
    "        (\"Sarah\", 26, \"Female\", \"Operations\", 4, 70000),\n",
    "        (\"Tom\", 36, \"Male\", \"Sales\", 14, 95000),\n",
    "        (\"Lucy\", 33, \"Female\", \"Engineering\", 11, 105000)]\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "df_pandas = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "\n",
    "# save the DataFrame to a CSV file\n",
    "df_pandas.to_csv('Company.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv(\"Company.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-----------+----------+------+\n",
      "| Name|Age|Gender| Department|Experience|Salary|\n",
      "+-----+---+------+-----------+----------+------+\n",
      "| John| 35|  Male|      Sales|        13| 90000|\n",
      "| Jane| 28|Female|  Marketing|         6| 75000|\n",
      "|  Bob| 42|  Male|Engineering|        20|120000|\n",
      "| Mary| 31|Female|      Sales|         9| 80000|\n",
      "|Steve| 45|  Male| Operations|        23|100000|\n",
      "|Emily| 29|Female|  Marketing|         7| 85000|\n",
      "|David| 39|  Male|Engineering|        17|110000|\n",
      "|Sarah| 26|Female| Operations|         4| 70000|\n",
      "|  Tom| 36|  Male|      Sales|        14| 95000|\n",
      "| Lucy| 33|Female|Engineering|        11|105000|\n",
      "+-----+---+------+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Gender', StringType(), True), StructField('Department', StringType(), True), StructField('Experience', IntegerType(), True), StructField('Salary', IntegerType(), True)])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Gender|count|\n",
      "+------+-----+\n",
      "|Female|    5|\n",
      "|  Male|    5|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## groupby\n",
    "\n",
    "df_pyspark.groupBy(\"Gender\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|     930000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({\"Salary\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK Machine learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySPARK Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-----------+----------+------+\n",
      "| Name|Age|Gender| Department|Experience|Salary|\n",
      "+-----+---+------+-----------+----------+------+\n",
      "| John| 35|  Male|      Sales|        13| 90000|\n",
      "| Jane| 28|Female|  Marketing|         6| 75000|\n",
      "|  Bob| 42|  Male|Engineering|        20|120000|\n",
      "| Mary| 31|Female|      Sales|         9| 80000|\n",
      "|Steve| 45|  Male| Operations|        23|100000|\n",
      "|Emily| 29|Female|  Marketing|         7| 85000|\n",
      "|David| 39|  Male|Engineering|        17|110000|\n",
      "|Sarah| 26|Female| Operations|         4| 70000|\n",
      "|  Tom| 36|  Male|      Sales|        14| 95000|\n",
      "| Lucy| 33|Female|Engineering|        11|105000|\n",
      "+-----+---+------+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"Age\", \"Experience\"],outputCol=\"Independent feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-----------+----------+------+-------------------+\n",
      "| Name|Age|Gender| Department|Experience|Salary|Independent feature|\n",
      "+-----+---+------+-----------+----------+------+-------------------+\n",
      "| John| 35|  Male|      Sales|        13| 90000|        [35.0,13.0]|\n",
      "| Jane| 28|Female|  Marketing|         6| 75000|         [28.0,6.0]|\n",
      "|  Bob| 42|  Male|Engineering|        20|120000|        [42.0,20.0]|\n",
      "| Mary| 31|Female|      Sales|         9| 80000|         [31.0,9.0]|\n",
      "|Steve| 45|  Male| Operations|        23|100000|        [45.0,23.0]|\n",
      "|Emily| 29|Female|  Marketing|         7| 85000|         [29.0,7.0]|\n",
      "|David| 39|  Male|Engineering|        17|110000|        [39.0,17.0]|\n",
      "|Sarah| 26|Female| Operations|         4| 70000|         [26.0,4.0]|\n",
      "|  Tom| 36|  Male|      Sales|        14| 95000|        [36.0,14.0]|\n",
      "| Lucy| 33|Female|Engineering|        11|105000|        [33.0,11.0]|\n",
      "+-----+---+------+-----------+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = output.select(\"Independent feature\", \"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|Independent feature|Salary|\n",
      "+-------------------+------+\n",
      "|        [35.0,13.0]| 90000|\n",
      "|         [28.0,6.0]| 75000|\n",
      "|        [42.0,20.0]|120000|\n",
      "|         [31.0,9.0]| 80000|\n",
      "|        [45.0,23.0]|100000|\n",
      "|         [29.0,7.0]| 85000|\n",
      "|        [39.0,17.0]|110000|\n",
      "|         [26.0,4.0]| 70000|\n",
      "|        [36.0,14.0]| 95000|\n",
      "|        [33.0,11.0]|105000|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "train_data,test_data = final_dataset.randomSplit([0.75,0.25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression(featuresCol=\"Independent feature\", labelCol=\"Salary\")\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([752.3148, 752.3148])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55254.62962962956"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------------------+\n",
      "|Independent feature|Salary|        prediction|\n",
      "+-------------------+------+------------------+\n",
      "|         [31.0,9.0]| 80000| 85347.22222222222|\n",
      "|        [39.0,17.0]|110000| 97384.25925925927|\n",
      "|        [42.0,20.0]|120000|101898.14814814818|\n",
      "+-------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12021.604938271592, 171808913.46593466)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError , pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-----------+----------+------+-------------------+\n",
      "| Name|Age|Gender| Department|Experience|Salary|Independent feature|\n",
      "+-----+---+------+-----------+----------+------+-------------------+\n",
      "| John| 35|  Male|      Sales|        13| 90000|        [35.0,13.0]|\n",
      "| Jane| 28|Female|  Marketing|         6| 75000|         [28.0,6.0]|\n",
      "|  Bob| 42|  Male|Engineering|        20|120000|        [42.0,20.0]|\n",
      "| Mary| 31|Female|      Sales|         9| 80000|         [31.0,9.0]|\n",
      "|Steve| 45|  Male| Operations|        23|100000|        [45.0,23.0]|\n",
      "|Emily| 29|Female|  Marketing|         7| 85000|         [29.0,7.0]|\n",
      "|David| 39|  Male|Engineering|        17|110000|        [39.0,17.0]|\n",
      "|Sarah| 26|Female| Operations|         4| 70000|         [26.0,4.0]|\n",
      "|  Tom| 36|  Male|      Sales|        14| 95000|        [36.0,14.0]|\n",
      "| Lucy| 33|Female|Engineering|        11|105000|        [33.0,11.0]|\n",
      "+-----+---+------+-----------+----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
