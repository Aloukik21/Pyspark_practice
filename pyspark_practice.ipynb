{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create a sample DataFrame\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Age': [25, 30, 35],\n",
    "        'City': ['New York', 'Paris', 'London']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# save the DataFrame to a CSV file\n",
    "df.to_csv('data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>35</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name  Age      City\n",
       "0    Alice   25  New York\n",
       "1      Bob   30     Paris\n",
       "2  Charlie   35    London"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d9aeab8790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing header column name\n",
    "df_pyspark = spark.read.option('header',\"true\").csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+\n",
      "|   Name|Age|    City|\n",
      "+-------+---+--------+\n",
      "|  Alice| 25|New York|\n",
      "|    Bob| 30|   Paris|\n",
      "|Charlie| 35|  London|\n",
      "+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Dataframe\n",
    "## Reading The Dataset\n",
    "## Checking the Datatypes of the Column(Schema)\n",
    "## Selecting Columns And Indexing\n",
    "## Check Describe option similar to Pandas\n",
    "## Adding Columns\n",
    "## Dropping columns\n",
    "## Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|   Name|    City|\n",
      "+-------+--------+\n",
      "|  Alice|New York|\n",
      "|    Bob|   Paris|\n",
      "|Charlie|  London|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','City']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'string'), ('City', 'string')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+------+\n",
      "|summary|   Name| Age|  City|\n",
      "+-------+-------+----+------+\n",
      "|  count|      3|   3|     3|\n",
      "|   mean|   null|30.0|  null|\n",
      "| stddev|   null| 5.0|  null|\n",
      "|    min|  Alice|  25|London|\n",
      "|    max|Charlie|  35| Paris|\n",
      "+-------+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Adding Columns in data frame\n",
    "df_pyspark=df_pyspark.withColumn('Age After 2 year',df_pyspark['Age']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+----------------+\n",
      "|   Name|Age|    City|Age After 2 year|\n",
      "+-------+---+--------+----------------+\n",
      "|  Alice| 25|New York|            27.0|\n",
      "|    Bob| 30|   Paris|            32.0|\n",
      "|Charlie| 35|  London|            37.0|\n",
      "+-------+---+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+--------+----------------+\n",
      "|New Name|Age|    City|Age After 2 year|\n",
      "+--------+---+--------+----------------+\n",
      "|   Alice| 25|New York|            27.0|\n",
      "|     Bob| 30|   Paris|            32.0|\n",
      "| Charlie| 35|  London|            37.0|\n",
      "+--------+---+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Rename the columns\n",
    "df_pyspark.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Handling missing value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id     name   department        title    salary\n",
      "0            1     John           HR      Manager  100000.0\n",
      "1            2    Alice  Engineering     Engineer   90000.0\n",
      "2            3      Bob      Finance          NaN   80000.0\n",
      "3            4  Charlie    Marketing  Coordinator       NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Define the column names for the DataFrame\n",
    "columns = ['employee_id', 'name', 'department', 'title', 'salary']\n",
    "\n",
    "# Define the data for the DataFrame\n",
    "data = [[1, 'John', 'HR', 'Manager', 100000],\n",
    "        [2, 'Alice', 'Engineering', 'Engineer', 90000],\n",
    "        [3, 'Bob', 'Finance', np.nan, 80000],\n",
    "        [4, 'Charlie', 'Marketing', 'Coordinator', np.nan]]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame to a CSV file\n",
    "df.to_csv('employee.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv(\"employee.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+-----------+--------+\n",
      "|employee_id|   name| department|      title|  salary|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "|          1|   John|         HR|    Manager|100000.0|\n",
      "|          2|  Alice|Engineering|   Engineer| 90000.0|\n",
      "|          3|    Bob|    Finance|       null| 80000.0|\n",
      "|          4|Charlie|  Marketing|Coordinator|    null|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+--------+\n",
      "|   name| department|      title|  salary|\n",
      "+-------+-----------+-----------+--------+\n",
      "|   John|         HR|    Manager|100000.0|\n",
      "|  Alice|Engineering|   Engineer| 90000.0|\n",
      "|    Bob|    Finance|       null| 80000.0|\n",
      "|Charlie|  Marketing|Coordinator|    null|\n",
      "+-------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop(\"employee_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------+--------+--------+\n",
      "|employee_id| name| department|   title|  salary|\n",
      "+-----------+-----+-----------+--------+--------+\n",
      "|          1| John|         HR| Manager|100000.0|\n",
      "|          2|Alice|Engineering|Engineer| 90000.0|\n",
      "+-----------+-----+-----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+-----------+--------+\n",
      "|employee_id|   name| department|      title|  salary|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "|          1|   John|         HR|    Manager|100000.0|\n",
      "|          2|  Alice|Engineering|   Engineer| 90000.0|\n",
      "|          3|    Bob|    Finance|       null| 80000.0|\n",
      "|          4|Charlie|  Marketing|Coordinator|    null|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "\n",
      "+-----------+-----+-----------+--------+--------+\n",
      "|employee_id| name| department|   title|  salary|\n",
      "+-----------+-----+-----------+--------+--------+\n",
      "|          1| John|         HR| Manager|100000.0|\n",
      "|          2|Alice|Engineering|Engineer| 90000.0|\n",
      "|          3|  Bob|    Finance|    null| 80000.0|\n",
      "+-----------+-----+-----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## any==how drop if there any null in row\n",
    "## all = drop all rows with all nan in rows\n",
    "## threshold if 2, if atleast 2 null values then it will be deleted\n",
    "## subset = remove from specific col\n",
    "\n",
    "df_pyspark.na.drop(how=\"any\", thresh=2).show()\n",
    "df_pyspark.na.drop(how=\"any\", subset=[\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+-----------+--------+\n",
      "|employee_id|   name| department|      title|  salary|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "|          1|   John|         HR|    Manager|100000.0|\n",
      "|          2|  Alice|Engineering|   Engineer| 90000.0|\n",
      "|          3|    Bob|    Finance|       null| 80000.0|\n",
      "|          4|Charlie|  Marketing|Coordinator|     0.0|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "\n",
      "+-----------+-------+-----------+-------------+--------+\n",
      "|employee_id|   name| department|        title|  salary|\n",
      "+-----------+-------+-----------+-------------+--------+\n",
      "|          1|   John|         HR|      Manager|100000.0|\n",
      "|          2|  Alice|Engineering|     Engineer| 90000.0|\n",
      "|          3|    Bob|    Finance|MIssing_value| 80000.0|\n",
      "|          4|Charlie|  Marketing|  Coordinator|    null|\n",
      "+-----------+-------+-----------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## fill missing value\n",
    "\n",
    "df_pyspark.na.fill(0).show()\n",
    "df_pyspark.na.fill(\"MIssing_value\", subset=[\"title\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+-----------+--------+\n",
      "|employee_id|   name| department|      title|  salary|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "|          1|   John|         HR|    Manager|100000.0|\n",
      "|          2|  Alice|Engineering|   Engineer| 90000.0|\n",
      "|          3|    Bob|    Finance|       null| 80000.0|\n",
      "|          4|Charlie|  Marketing|Coordinator|    null|\n",
      "+-----------+-------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing mean\n",
    "# Calculate the mean of each column\n",
    "from pyspark.sql.functions import mean\n",
    "means = df_pyspark.select([mean(c) for c in df_pyspark.columns]).first()\n",
    "# Calculate the mean of the \"salary\" column\n",
    "mean_salary = df_pyspark.select(mean('salary')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(avg(employee_id)=2.5, avg(name)=None, avg(department)=None, avg(title)=None, avg(salary)=90000.0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.na.fill(mean_salary, subset=[\"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter operations\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
